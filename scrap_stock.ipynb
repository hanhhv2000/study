{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b89ff4abb5b5b53a256161c73b9997811481b29d5077211419309f62bda64171"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "import time\n",
    "\n",
    "#binary = FirefoxBinary('path/to/binary')\n",
    "browser = webdriver.Chrome(executable_path= \"C:\\chromedriver.exe\")\n",
    "browser.get(\"https://www.stockbiz.vn/IndicesStats.aspx\")\n",
    "\n",
    "target_page = 1\n",
    "\n",
    "# Display time started\n",
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n",
    "# Run main web scraping function\n",
    "full_scrape(target_page)\n",
    "\n",
    "# Display time concluded\n",
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n",
    "# elem = browser.find_element_by_xpath('//*[@id=\"ctl00_webPartManager_wp267165551_wp1192412521_callbackData\"]/div')\n",
    "# # button = browser.find_element_by_class_name(\"pageNavigation\")\n",
    "# # print(button)\n",
    "# #button.click()\n",
    "# #time.sleep(5)\n",
    "\n",
    "# elem.click()\n",
    "# time.sleep(0.2)\n",
    "\n",
    "# elem = browser.find_element_by_xpath(\"//*\")\n",
    "# print(button.get_attribute(\"outerHTML\"))\n",
    "\n",
    "def gen_hcp_dict():\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='table-head']\")))\n",
    "    hcp_name =  driver.find_element_by_xpath(\"//div[@class='table-head']\").text    \n",
    "    all_fields = driver.find_elements_by_xpath(\"//td[@class='no-border table-data']\") # Using find elementS since there are multiple elements    \n",
    "    hcp_data = []\n",
    "    # Extract test for every field within the table\n",
    "    for field in all_fields:\n",
    "        hcp_data.append(field.text)\n",
    "    hcp_dict = {}\n",
    "    # Assign extracted data into respective key-value pairs\n",
    "    hcp_dict['name'] = hcp_name\n",
    "    hcp_dict['reg_number'] = hcp_data[0]\n",
    "    # hcp_data[1] is just a blank space, so it can be ignored\n",
    "    hcp_dict['reg_date'] = hcp_data[2]\n",
    "    hcp_dict['reg_end_date'] = hcp_data[3]\n",
    "    hcp_dict['reg_type'] = hcp_data[4]\n",
    "    hcp_dict['practice_status'] = hcp_data[5]\n",
    "    hcp_dict['cert_start_date'] = hcp_data[6]\n",
    "    hcp_dict['cert_end_date'] = hcp_data[7]\n",
    "    hcp_dict['qualification'] = hcp_data[8]\n",
    "    hcp_dict['practice_place_name'] = hcp_data[9]\n",
    "    hcp_dict['practice_place_address'] = hcp_data[10]\n",
    "    hcp_dict['practice_place_phone'] = hcp_data[11]\n",
    "    \n",
    "    return hcp_dict\n",
    "#==========================================================================================================#\n",
    "\n",
    "def get_absolute_last_page():\n",
    "    # Find all elements with pagination class (since it contains \n",
    "    # page numbers)\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, \"//a[@class='pagination']\")))\n",
    "    all_pages = driver.find_elements_by_xpath(\"//a[@class='pagination']\")\n",
    "\n",
    "    # Get final element, which corresponds to 'Last' hyperlink\n",
    "    # (which will go to last page number)\n",
    "    last_elem = all_pages[-1].get_attribute('href')\n",
    "\n",
    "    # Keep only the number of last page\n",
    "    last_page_num = int(re.sub(\"[^0-9]\", \"\", last_elem))\n",
    "    \n",
    "    return last_page_num\n",
    " #==========================================================================================================#   \n",
    "def get_current_pagination_range():\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, \"//a[@class='pagination']\")))\n",
    "    # Get all elements related to pagination\n",
    "    all_pages = driver.find_elements_by_xpath(\"//a[@class='pagination']\")\n",
    "    # Implicit wait to allow page to load\n",
    "    driver.implicitly_wait(1)\n",
    "    # Find numbers of pagination range, and append to list\n",
    "    pagination_range_on_page = []\n",
    "    for elem in all_pages:\n",
    "        # Only extracting numeric values of pagination range\n",
    "        if elem.text.isnumeric():\n",
    "            pagination_range_on_page.append(int(elem.text))\n",
    "            driver.implicitly_wait(1)\n",
    "        else:\n",
    "            pass\n",
    "    driver.implicitly_wait(1)\n",
    "    return pagination_range_on_page\n",
    "#==========================================================================================================#\n",
    "def locate_target_page(target_page):\n",
    "    last_page_num = get_absolute_last_page()\n",
    "    # Find midway point of all search results page\n",
    "    midway_point = last_page_num/2\n",
    "    # If target page is in first half, start clicking from the start\n",
    "    if target_page < midway_point: \n",
    "        current_page_num = get_current_page()\n",
    "        if current_page_num == target_page:\n",
    "            pass\n",
    "        else:            \n",
    "            pagination_range = get_current_pagination_range()\n",
    "            while target_page not in pagination_range:\n",
    "                driver.implicitly_wait(1)\n",
    "               # If target page is not in pagination range, keep \n",
    "               # clicking last pagination number to go down the list\n",
    "                click_last_pagination_num(pagination_range) \n",
    "                current_page_num = get_current_page()\n",
    "                pagination_range = get_current_pagination_range()\n",
    "                driver.implicitly_wait(1)\n",
    "            else:\n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.LINK_TEXT, f\"{target_page}\"))).click() # Once target          page is in pagination page, go to the target page\n",
    "    # If target page is in later half of list, go to Last page and      \n",
    "    # move in reverse (This saves alot of time)\n",
    "    else: \n",
    "        WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.LINK_TEXT, 'Last'))).click()\n",
    "        time.sleep(sleeptime)\n",
    "        current_page_num = get_current_page()\n",
    "        if current_page_num == target_page:\n",
    "            pass\n",
    "        else:           \n",
    "            pagination_range = get_current_pagination_range()\n",
    "            while target_page not in pagination_range:\n",
    "                driver.implicitly_wait(2)\n",
    "                click_first_pagination_num(pagination_range)\n",
    "                current_page_num = get_current_page()\n",
    "                pagination_range = get_current_pagination_range()\n",
    "            else:\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, f\"{target_page}\"))).click() \n",
    "                # Once target page is in pagination page, go to the target page\n",
    "#==========================================================================================================#\n",
    "def full_scrape(target_page):\n",
    "\n",
    "    last_page_num = get_absolute_last_page()\n",
    "    driver.implicitly_wait(1)\n",
    "    \n",
    "    while target_page != last_page_num:\n",
    "        locate_target_page(target_page)\n",
    "        print('Starting with target page ' + str(target_page))\n",
    "        \n",
    "        # Retrieve HTML from search page\n",
    "        target_page_html = driver.find_element_by_xpath(\"//body\").get_attribute('outerHTML')\n",
    "        driver.implicitly_wait(1)\n",
    "        \n",
    "        # Find list of all IDs on that page, and keep the unique IDs\n",
    "        all_ids = re.findall(\"P[0-9]{5}[A-Z]{1}\", target_page_html)\n",
    "        id_list = list(dict.fromkeys(all_ids))\n",
    "\n",
    "        for index, hcp_id in enumerate(id_list): # Tracking the healthcare professional (HCP)'s ID\n",
    "            # Click 'View More Details' link to access details page \n",
    "            # for that professional with the specific ID\n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//a[contains(@onclick,'{hcp_id}')]\"))).click()\n",
    "            # Scrape data from details page into a dictionary\n",
    "            hcp_dict = gen_hcp_dict()\n",
    "            # Convert dict to pandas dataframe (Need to set an index \n",
    "            # since we are passing scalar values)\n",
    "            df_hcp_dict = pd.DataFrame(hcp_dict, index=[0])\n",
    "            # Append df row directly to existing master list csv\n",
    "            df_hcp_dict.to_csv(f'{file_name}', mode='a', header=False)\n",
    "            # Print row that was just scraped (To track progress)\n",
    "            print(f'Scraped row {index+1} of page {target_page}')\n",
    "            # Update (+1) target page after successfully scrapping\n",
    "            # all 10 records on page\n",
    "            if index == len(id_list):\n",
    "                print(f'Completed scraping for page {target_page}')\n",
    "                target_page += 1\n",
    "                print('Updated target page ' + str(target_page))\n",
    "            else:\n",
    "                pass\n",
    "            # Head back to homepage by clicking 'Back to Search \n",
    "            # Results' link\n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.LINK_TEXT, 'Back to Search Results'))).click() \n",
    "            # Go to latest updated target page\n",
    "            locate_target_page(target_page)\n",
    "    # Define steps when program reaches last page (essentially \n",
    "    # similar to the steps above)\n",
    "    else:\n",
    "        locate_target_page(target_page)\n",
    "        print('Working on last page')\n",
    "        target_page_html = driver.find_element_by_xpath(\"//body\").get_attribute('outerHTML')\n",
    "        driver.implicitly_wait(1)\n",
    "        all_ids = re.findall(\"P[0-9]{5}[A-Z]{1}\", target_page_html)\n",
    "        id_list = list(dict.fromkeys(all_ids))\n",
    "        for index, hcp_id in enumerate(id_list): \n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//a[contains(@onclick,'{hcp_id}')]\"))).click()                 hcp_dict = gen_hcp_dict()\n",
    "            df_hcp_dict = pd.DataFrame(hcp_dict, index=[0])\n",
    "            df_hcp_dict.to_csv('master_list.csv', mode='a', header=False)\n",
    "            print(f'Scraped row {index+1} of {target_page}')\n",
    "            if index == len(id_list)-1:\n",
    "                print(f'Completed scraping for page {target_page}')\n",
    "                print('Mission Complete')\n",
    "            else:\n",
    "                pass\n",
    "            WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.LINK_TEXT, 'Back to Search Results'))).click() \n",
    "            locate_target_page(target_page)\n",
    "#==========================================================================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}