{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b89ff4abb5b5b53a256161c73b9997811481b29d5077211419309f62bda64171"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'sdf'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-82f39e181a37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mdf4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mdf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m'sdf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     dict = {'3': 'kho',\n\u001b[0;32m     96\u001b[0m         \u001b[1;34m'4'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ma_da'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'sdf'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "# driver = webdriver.Chrome()\n",
    "import time\n",
    "import csv\n",
    "\n",
    "s_file ='data.csv'\n",
    "# outfile = open('table3.csv','w',newline='', encoding ='utf-8')\n",
    "# writer = csv.writer(outfile)  \n",
    "\n",
    "def get_row_count(tbl):\n",
    "        return len(tbl.find_elements_by_tag_name(\"tr\")) \n",
    "\n",
    "def row_data(tbl,row_number):\n",
    "    # if(row_number == 0):\n",
    "    #     raise Exception(\"Row number starts from 1\")\n",
    "    # row_number = row_number + 1\n",
    "    row = tbl.find_elements_by_xpath(\"//tr[\" + str(row_number) + \"]/td\")#.get_attribute(\"innerHTML\")\n",
    "    rData = []\n",
    "    for webElement in row :\n",
    "        # if webElement.text != \"\":\n",
    "             rData.append(webElement.text.strip())\n",
    "    return rData\n",
    "# options = Options()\n",
    "# options.add_argument(\"start-maximized\")\n",
    "# options.add_argument(\"disable-infobars\")\n",
    "# options.add_argument(\"--disable-extensions\")\n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "# driver = webdriver.Chrome(chrome_options=options,executable_path=r'c:\\chromedriver.exe')\n",
    "# driver.maximize_window()\n",
    "driver.implicitly_wait(10)\n",
    "driver.get(\"http://10.70.105.32/DATTDT/dautu/thongtinduan.aspx\")\n",
    "time.sleep(1)\n",
    "# WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[@class='mfp-close']\"))).click()\n",
    "\n",
    "inputElement = driver.find_element_by_id(\"txtTenDangNhap\")\n",
    "inputElement.send_keys(Keys.CONTROL, \"a\") # Select all pre-existing text/input value\n",
    "inputElement.send_keys('03836')\n",
    "# inputElement.send_keys(Keys.ENTER)\n",
    "time.sleep(1)\n",
    "inputElement = driver.find_element_by_id(\"txtMatKhau\")\n",
    "inputElement.send_keys(Keys.CONTROL, \"a\") # Select all pre-existing text/input value\n",
    "inputElement.send_keys('Vk@#20002004')\n",
    "time.sleep(1)\n",
    "inputElement.send_keys(Keys.ENTER)\n",
    "# WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@id='col-md-12']//button[@id='btnDangNhap']\")))\n",
    "# WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//div[@id='col-md-12']//button[@id='btnDangNhap']\"))).click()\n",
    "time.sleep(1)\n",
    "inputElement = driver.find_element_by_id(\"txtTuKhoa\")\n",
    "# inputElement.click()\n",
    "inputElement.send_keys(Keys.CONTROL, \"a\") # Select all pre-existing text/input value\n",
    "inputElement.send_keys('cl')\n",
    "inputElement.send_keys(Keys.ENTER)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//table[@class='dataTable']\")))\n",
    "WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='k-pager-wrap k-grid-pager k-widget k-floatwrap']/a[@class ='k-link k-pager-nav k-pager-last']\")))\n",
    "a_ref = driver.find_element_by_xpath(\"//div[@class='k-pager-wrap k-grid-pager k-widget k-floatwrap']/a[@class ='k-link k-pager-nav k-pager-last']\")\n",
    "a_= a_ref.get_attribute('data-page')\n",
    "# print(a_)\n",
    "a_count = int(a_) + 1\n",
    "outfile = open(s_file, 'w', newline='', encoding= 'utf-8')\n",
    "writer = csv.writer(outfile)\n",
    "row=[]\n",
    "for j in range(1,2):       \n",
    "    time.sleep(0.5)\n",
    "    t =[]\n",
    "    # tag_li = driver.find_element_by_xpath(\".//span[@class ='k-link k-pager-nav']\")\n",
    "    # print(tag_li.text())\n",
    "    if j > 1 :   \n",
    "        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='k-pager-wrap k-grid-pager k-widget k-floatwrap']/a[@class='k-link k-pager-nav']/span[@class='k-icon k-i-arrow-e']\"))).click()\n",
    "        # WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='k-pager-wrap k-grid-pager k-widget k-floatwrap']/ul[@class='k-pager-numbers k-reset']/li[\"+ str(j+1) +\"]/a\"))).click()\n",
    "    # WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='k-grid-content k-auto-scrollable']/table[@class='k-selectable']/tbody\")))\n",
    "    # WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//table[@class='k-selectable']/tbody\")))\n",
    "    # tr_ = driver.find_element_by_css_selector(\"td:nth-child(5)\").text\n",
    "    # print(tr_)\n",
    "    time.sleep(0.5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml') # tried: 'html.parser\n",
    "    table2 = soup.find_all('table')[2]\n",
    "    df2 = pd.read_html(str(table2))\n",
    "    df4 = df2[0].loc[:,3:]\n",
    "    ten = df4.columns.values[1] \n",
    "    df4.columns.values[2]  = 'sdf'\n",
    "    dict = {'3': 'kho',\n",
    "        '4': 'ma_da',\n",
    "        '5': 'dia_chi'}\n",
    "  \n",
    "# call rename () method\n",
    "    df4.rename(columns=dict,  inplace=True)\n",
    "    # data_1 = pd.DataFrame(df4, columns=['kho', 'ma_da', 'dia_chi'])\n",
    "\n",
    "    # df2 = pd.dataframe\n",
    "    # df2[0].columns.str.match('Unnamed')\n",
    "    # df2[0].loc[:, ~df2[0].columns.str.match('Unnamed')]\n",
    "    \n",
    "    # table3 = soup.find_all('table')[3]\n",
    "    # df3 = pd.read_html(str(table3))\n",
    "    # # df3[0].columns.str.match('Unnamed')\n",
    "    # # df3[0].loc[:, ~df3[0].columns.str.match('Unnamed')]\n",
    "    # result = pd.concat([df2[0], df3[0]], axis=1) #  lấy data 1 trang\n",
    "    # result.loc[:, ~result.columns.str.match('Unnamed')] #  lấy data 1 trang\n",
    "\n",
    "    # t.append(result)\n",
    "    # t\n",
    "    # for d in t:\n",
    "    #     print(d)\n",
    "\n",
    "    # processeddate=soup.find('span', attrs={'class':'k-link k-pager-nav'}).text\n",
    "    # # print(soup.tbody)\n",
    "    # print(processeddate)\n",
    "    # root = soup.body\n",
    "\n",
    "    # root_childs = [e.name for e in root.children if e.name is not None]\n",
    "    # print(root_childs)\n",
    "    # tbl_1 = soup.find('table', {'class': 'k-selectable'})\n",
    "    # # arr =[]\n",
    "    # # for row in tbl_1.find_all('tr'):\n",
    "    # #     arr.append(row)\n",
    "    # # # print(arr)\n",
    "    \n",
    "    # # table = soup.find('table', class ='k-selectable')\n",
    "    # if tbl_1 is not None:\n",
    "    #     # for link in tbl_1:\n",
    "    #     datalist = []\n",
    "    #     for row in tbl_1.find_all('tr'):\n",
    "    #         list_of_cells = []\n",
    "    #         for cell in row.find_all(['td']):\n",
    "    #             # if row.find_all(['td']).index(cell) != 0 or row.find_all(['td']).index(cell) != 1 or row.find_all(['td']).index(cell) != 2:\n",
    "    #             # text = cell.text.replace('\\r\\n', '').replace('\\r','').replace('\\n','').replace('xa0','').strip()\n",
    "    #             list_of_cells.append(cell.text) \n",
    "    #         #  soup2 = BeautifulSoup(page_source, 'lxml') \n",
    "    #         #  table = soup2.find_all('table')[2]\n",
    "            \n",
    "    #         #  df = pd.read_html(str(table),header=0)\n",
    "    #         # #  if df[0].text != 'NaN':\n",
    "    #         #  print(df)\n",
    "    #         datalist.append(list_of_cells)\n",
    "    #     t.append(datalist)\n",
    "    # print(datalist)\n",
    "# df = pd.DataFrame(t)\n",
    "# print(df)    \n",
    "# df.to_csv('Data.csv', sep=',', encoding='utf-8-sig',index = False )\n",
    "    # rows=[]\n",
    "    # for item in t:\n",
    "    #     col=[]\n",
    "    #     print(item)\n",
    "    #     for ten in item:\n",
    "    #     # writer.writerow(item)\n",
    "    #     # text = item.replace('\\r\\n', '').replace('\\r','').replace('\\n','').replace('xa0','').strip()\n",
    "    #         col.append(ten)\n",
    "    #         print(ten)            \n",
    "    #     rows.append(col)\n",
    "    # print(rows) \n",
    "    # print(' '.join(row)) \n",
    "# for r in row :\n",
    "#     writer.writerow(r)\n",
    "#     print(' '.join(r))    \n",
    "        \n",
    "\n",
    "outfile.close()   \n",
    "driver.close()\n",
    "driver.quit()\n",
    "print(df4)\n",
    "print(ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "\n",
    "html = \"\"\"<span class=\"text-border tooltips\" data-original-title=\"Happiness 84%\n",
    "  Sadness 80%\n",
    "  \" data-placement=\"left\" data-toggle=\"tooltip\">More stats</span>,\n",
    "  <span class=\"text-border tooltips\" data-original-title=\"Happiness 70%\n",
    "  Sadness 59%\n",
    "  \" data-placement=\"left\" data-toggle=\"tooltip\">More stats</span>\"\"\"\n",
    "soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "all_rows = []\n",
    "for span in soup.find_all('span'):\n",
    "    title_eles = re.split(' |\\n', span['data-original-title'])\n",
    "    print(title_eles)\n",
    "    title_eles = list(filter(None, title_eles))\n",
    "    print(title_eles)\n",
    "    row = dict(itertools.zip_longest(title_eles[::2], title_eles[1::2], fillvalue=\"\"))\n",
    "    # print(row)\n",
    "    all_rows.append(row)\n",
    "pd.DataFrame(all_rows)\n",
    "# pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "dates = pd.date_range('1/1/2000', periods=8)\n",
    "df = pd.DataFrame(np.random.randn(8, 4),\n",
    "                  index=dates, columns=['A', 'B', 'C', 'D'])\n",
    "print(df)\n",
    "# s = df['A']\n",
    "# print('=========================')\n",
    "# print(s)\n",
    "# t=s[dates[5]]\n",
    "# print('=========================')\n",
    "# print(t)\n",
    "# df[['B', 'A']] = df[['A', 'B']] # đổi giá trị cột \n",
    "# print('=========================')\n",
    "# print(df)\n",
    "sa = pd.Series([1, 2, 3], index=list('abc'))\n",
    "dfa = df.copy() # bản copy arr 2\n",
    "r= sa.b\n",
    "sa.a = 5\n",
    "print('=========================')\n",
    "# print(sa)\n",
    "# print('=========================')\n",
    "# print(dfa.A) # gọi data cột \n",
    "# dfa.A = list(range(len(dfa.index)))\n",
    "# print(dfa.A)\n",
    "# print(dfa)\n",
    "print(df.iloc[0:5])\n",
    "print(df.loc(datetime.datetime.strptime('2000-01-03','%Y-%m-%d')))\n",
    "# t = np.random.randn()\n",
    "# t\n",
    "# dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}